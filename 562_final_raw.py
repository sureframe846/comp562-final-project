# -*- coding: utf-8 -*-
"""562_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nSr-HFJ9tuCM0Fk4Kg9GGWK5uW0M8rFF

# Exploring Machine Learning Methods for 
# Handwritten Digit Classification

#### Trent Avent , Eliot McGinnis

# Introduction
For our project, we implemented a variety of both supervised and unsupervised machine learning methods in order to classify handwriten digits. Our data is taken from the MNIST library and includes 70000 images of handwriten numbers in the form of 28 x 28 arrays of RGB values. This data has been studied by many machine learning experts and is the subject of papers testing various algorithms. These papers have examined various linear classifiers, k-nearest neighbors with various choices of k, neural networks an other models. We seek to apply methods which we have learned in this class to the data, to find an optimal method of the classical approaches. We fit both unsupervised and supervised methods to the data, and gain insight into the nature of the models in the process. Because the data is pixelated, there is some variation in model performance- some models handle this form of data better than others. 



For preprocessing, we split the data into 60000 training instances and 10000 test cases. In order to proceed, we chose to scale the data points between zero and one by dividing all of the predictor values by 255, the previous scale.
"""

import pandas as pd
import numpy as np

import tensorflow as tf
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_train = x_train / 255.0
x_test = x_test / 255.0
n_samples = len(x_train)
x1 = x_train.reshape((n_samples, -1))
n_samples2 = len(x_test)
x2 = x_test.reshape((n_samples2, -1))

for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(y_train[i])
plt.tight_layout()

"""### Principal Component Analysis

We use the unsupervised technique principal component analysis to reduce the dimension of the data for our exploratory analysis. The first two principal components contain a large part of the variation of the data, and by plotting these components and coloring by digit value, we see some relationships in the data. Principal component analysis is an excellent way to visualize our data.
"""

# PCA

from sklearn.decomposition import PCA
import pandas as pd
from skimage import io, color
pca = PCA(n_components=2)
n_samples = len(x_train)
x1 = x_train.reshape((n_samples, -1))
pc = pca.fit_transform(x1)
pc = pd.concat([pd.DataFrame(pc, columns = ['pc1', 'pc2']), pd.Series(y_train, name = "y")],1)

from matplotlib import pyplot as plt

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('PC1', fontsize = 15)
ax.set_ylabel('PC2', fontsize = 15)

y_values = [0,1,2,3,4,5,6,7,8,9]
colors = ['salmon', 'peru', 'darkolivegreen','deepskyblue',
          'indigo','gold','lime', 'blue', 'magenta']
for target, color in zip(y_values,colors):
    indicesToKeep = pc['y'] == target
    ax.scatter(pc.loc[indicesToKeep, 'pc1'], pc.loc[indicesToKeep, 'pc2'],
         c = color, s = 50)
ax.legend(y_values)
ax.grid()

"""Using only the first two principal components, there is no clear decision boundary. There is overlap among all of the classes, and our models will need to overcome this. This is natural- perhaps messier handwriting can lead to digits which are harder to recognize for both machine learning models and humans.

# Supervised Approaches:

### Support Vector Machines

We fit both linear and radial basis function kernel SVM to our data and examine the results. 

#### Linear Kernel SVM

We first implemented an SVM model using the sklearn library. We found the best choice of C to be 1. Below, we see the results and a matrix giving the classification rates for each class.
"""

from sklearn import svm

svm_classifier = svm.SVC(kernel = "linear", C = 1) #decision_function_shape = 'ovo')
linear_svm = svm_classifier.fit(x1,y_train)

pred_linsvm = linear_svm.predict(x2)
print("Linear SVM test error:")
print(np.mean(pred_linsvm != y_test))
pd.crosstab(pred_linsvm, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns" )

pd.Series(np.diag(pd.crosstab(pred_linsvm, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns" )))

"""This matrix shows the percentage of each predicted class that falls under each of the actual classes of digits. The diagonals of this matrix show the rate of correct classifications for the respective digits. We see that the model had the most rouble with classifying the digits 5 and 8."""

from sklearn import svm
svm_classifier2 = svm.SVC(kernel = "rbf", C = 1, gamma = "scale")
rbf_svm = svm_classifier2.fit(x1,y_train)

pred_rbfsvm = rbf_svm.predict(x2)
print("RBF kernel SVM test error:")
print(np.mean(pred_rbfsvm != y_test))
pd.crosstab(pred_rbfsvm, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns" )

pd.Series(np.diag(pd.crosstab(pred_rbfsvm, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns" ) ))

"""### Multiclass Logistic Regression

We use multiclass logistic regression from the sklearn library on our data. We see the results below:
"""

from sklearn.linear_model import LogisticRegression
logit = LogisticRegression(multi_class="ovr", max_iter = 1000)
model = logit.fit(x1, y_train)

logit_pred = model.predict(x2)
print("Multiclass Logistic Regression Test error: ")
print(np.mean(logit_pred != y_test))
pd.Series(np.diag(pd.crosstab(logit_pred, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns" )))

pd.Series(np.diag(pd.crosstab(logit_pred, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns" )))

"""The matrix tells us that the logistic model performs worst on digits 5 and 8. It performs overall worse than the SVM methods.

### K-Nearest Neighbors

We fit a KNN model to our data. We found the best model to be using K=5. Note that we chose an odd K to ensure that there were no ties for classification in the data along class boundaries.
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5 )
knn_model = knn.fit(x1,y_train)

pred_knn = knn_model.predict(x2)
print("KNN- test error:")
print(np.mean(pred_knn != y_test))
pd.Series(np.diag(pd.crosstab(pred_knn, y_test, rownames = ['predicted'], colnames = ['actual'], normalize = "columns")))

"""### Unsupervised Approaches: Other Methods


#### K-Means Clustering
We fitted a KMeans model with 10 clusters to the training data. We received poor results as seen in the table. There were no distinct trends apparent, so there was issues properly labelling the clusters. This is similar to the uncertain results that we found by plotting the data on the principal components. We must consider that k-means uses a distance function that may not be appropriate for working with pixel data.
"""

# K means with 10 clusters
from sklearn.cluster import KMeans
X = x_train.reshape(len(x_train),-1)
Y = y_train
km = KMeans(init='k-means++',n_clusters=10).fit(X)

kmpred = km.predict(x_test.reshape(len(x_test),-1))
pd.crosstab(kmpred, y_test, rownames = ['cluster'], colnames = ['actual'])

"""### Neural Network
Finally, we tried a neural network with 2 hidden layers with 128 and 10 nodes each. The neural net is fit over 10 epochs in order to further increase the accuracy.
"""

# Two layer neural net with 128 and 10 nodes
from tensorflow import keras

neuralnet = keras.Sequential([
    keras.layers.Flatten(input_shape = (28,28)),
    keras.layers.Dense(128),
    keras.layers.Dense(10)
])

neuralnet.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
neuralnet.fit(x_train, y_train, epochs = 10)
test_loss, test_acc = neuralnet.evaluate(x_test, y_test, verbose = 2)

print('\nTest accuracy:', test_acc)
probability_model = tf.keras.Sequential([neuralnet, 
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(x_test)

nn_pred = np.arange(len(x_test))
for i in range(len(x_test)):
  pred_label = np.argmax(predictions[i])
  nn_pred[i] = pred_label
#nn_df = pd.DataFrame([nn_pred,y_test])
pd.Series(np.diag(pd.crosstab(nn_pred, y_test, rownames = ['cluster'], colnames = ['actual'], normalize = "columns")))

"""The final test accuracy of the model is quite good, and the diagonal of the table shows the percentage of properly classified samples for each digit."""

for i in range(25):
    ax = plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_test[i], cmap=plt.cm.binary)
    predicted_label = np.argmax(predictions[i])
    if predicted_label == y_test[i]:
        color = 'green'
        label = ": Correct"
    else:
        color = 'red'
        label = ": Incorrect"
    ax.set_xlabel(str(predicted_label) + label, color=color)
plt.tight_layout()

"""# Results
Overall the neural network was the highest performing unsupervized model.  Perhaps the model can be improved by testing different combinations and sizes of layers.

For the supervised methods, we found that ...
"""